{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850f4fa0-65f9-400a-8d80-ab3c0e8ebbf7",
   "metadata": {},
   "source": [
    "# Q1\n",
    "## Simple Linear Regression vs. Multiple Linear Regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01987c-6284-4331-9cec-2b2b9f498a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression involves modeling the relationship between two variables: one dependent variable (Y) and one independent variable (X), assuming a linear relationship between them. \n",
    "The equation for simple linear regression is given by: Y = β0 + β1X + ε, where β0 and β1 are the regression coefficients, and ε is the error term.\n",
    "--Example of Simple Linear Regression: Suppose we want to model the relationship between the number of hours studied (X) and the exam score (Y) of a group of students. \n",
    "  We collect data on the number of hours each student studied and their corresponding exam scores. We can then perform simple linear regression to estimate the regression coefficients and make predictions about the exam scores based on the number of hours studied.\n",
    "\n",
    "Multiple linear regression, on the other hand, involves modeling the relationship between one dependent variable (Y) and multiple independent variables (X1, X2, X3, ..., Xn), assuming a linear relationship between them. \n",
    "The equation for multiple linear regression is given by: Y = β0 + β1X1 + β2X2 + ... + βnXn + ε, where β0, β1, β2, ..., βn are the regression coefficients, and ε is the error term.\n",
    "--Example of Multiple Linear Regression: Suppose we want to model the housing prices (Y) based on various features such as the size of the house (X1), the number of bedrooms (X2), the location (X3), and the age of the house (X4). \n",
    "  We collect data on these features for a set of houses and perform multiple linear regression to estimate the regression coefficients and make predictions about the housing prices based on the given features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c47aee-3d50-47b2-8404-fe4374165795",
   "metadata": {},
   "source": [
    "# Q2\n",
    "###### Linearity: The relationship between the dependent variable and the independent variable(s) should be linear, meaning that the change in the dependent variable is proportional to the change in the independent variable(s).\n",
    "###### Independence of Errors: The errors (residuals) should be independent, meaning that the value of the error for one observation should not depend on the value of the error for any other observation.\n",
    "###### Homoscedasticity: The errors should have constant variance, meaning that the variance of the errors should be the same for all values of the independent variable(s).\n",
    "\n",
    "###### Normality of Errors: The errors should be normally distributed, meaning that the distribution of errors should follow a bell-shaped normal distribution.\n",
    "\n",
    "###### Multicollinearity: In multiple linear regression, the independent variables should not be perfectly correlated with each other, as it can cause instability in the estimation of regression coefficients.\n",
    "\n",
    "#### Checking Assumptions in a Given Dataset:\n",
    "###### There are several methods to check whether the assumptions of linear regression hold in a given dataset, including:\n",
    "\n",
    "###### Plotting Residuals: Plotting the residuals (errors) against the predicted values or the independent variable(s) can provide insights into the linearity, homoscedasticity, and presence of outliers in the data.\n",
    "\n",
    "###### Checking Normality: Plotting the residuals on a histogram or a Q-Q plot can help assess the normality of errors. Additionally, statistical tests such as the Anderson-Darling test or the Shapiro-Wilk test can be used to formally test the normality of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8788dfa-070d-4107-9dc3-a3c69b97a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "In a linear regression model, the slope (β1) represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X), while holding all other variables constant. The intercept (β0) represents the estimated value of the dependent variable (Y) when the independent variable (X) is zero.\n",
    "\n",
    "For example, let's consider a real-world scenario of predicting house prices based on the size of the house (in square feet) as the independent variable (X) and the price of the house (in dollars) as the dependent variable (Y) in a simple linear regression model.\n",
    "\n",
    "Interpretation of the slope (β1): If the slope is estimated to be 50, it means that for every one-unit increase in the size of the house (in square feet), the predicted price of the house (in dollars) is expected to increase by 50, assuming all other factors remain constant.\n",
    "\n",
    "Interpretation of the intercept (β0): If the intercept is estimated to be 100, it means that when the size of the house (in square feet) is zero (which is not practically meaningful in this context), the predicted price of the house (in dollars) is expected to be 100.\n",
    "\n",
    "So, in this example, the slope represents the rate of change in house prices for each additional square foot of the house, and the intercept represents the estimated price of the house when the size of the house is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ff59fe-f941-4b8c-a66b-84bf76147d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4\n",
    "Gradient descent is an optimization algorithm used in machine learning to minimize the cost or loss function of a model during the training process. \n",
    "It involves iteratively updating the model's parameters in the direction of the negative gradient of the cost function to find the optimal values of the parameters that minimize the cost function. \n",
    "The gradient is the vector of partial derivatives of the cost function with respect to each parameter, and it determines the direction of the steepest increase in the cost function.\n",
    "By taking steps proportional to the negative gradient, gradient descent gradually approaches the minimum of the cost function, allowing the model to learn the optimal values of the parameters for making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef327e-d91a-47f5-ab5d-832f9d7a9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "Multiple linear regression is a statistical technique used to model the relationship between a single dependent variable (Y) and multiple independent variables (X1, X2, X3, ..., Xn), assuming a linear relationship between them. \n",
    "The equation for multiple linear regression is given by: Y = β0 + β1X1 + β2X2 + ... + βnXn + ε, where β0 is the intercept, β1, β2, ..., βn are the regression coefficients for each independent variable, X1, X2, ..., Xn are the values of the independent variables, and ε is the error term.\n",
    "\n",
    "The main difference between multiple linear regression and simple linear regression is that multiple linear regression involves modeling the relationship between a dependent variable and multiple independent variables, whereas simple linear regression involves modeling the relationship between a dependent variable and only one independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa867a6b-2cb1-40f5-87a3-dd92076d0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6\n",
    "Multicollinearity refers to the presence of high correlation among two or more independent variables in a multiple linear regression model. \n",
    "It can cause issues in interpreting the individual effects of each independent variable on the dependent variable, as the impact of one variable may be confounded by the presence of other highly correlated variables.\n",
    "\n",
    "To detect multicollinearity, common methods include calculating the variance inflation factor (VIF) for each independent variable, where a VIF value greater than 10 is generally considered indicative of multicollinearity. \n",
    "Another method is to calculate the condition number, where a condition number greater than 30 may indicate multicollinearity.\n",
    "\n",
    "To address multicollinearity, one approach is to remove one of the correlated variables from the model.\n",
    "Another approach is to use regularization techniques such as Ridge Regression or Lasso Regression, which introduce a penalty term to the regression equation to reduce the impact of multicollinearity. \n",
    "These regularization techniques can help to stabilize the estimates of the regression coefficients and mitigate the effects of multicollinearity on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1e404-5668-44d6-ad08-5b84c6ccb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7\n",
    "Polynomial regression is a form of regression analysis that models the relationship between a dependent variable (Y) and an independent variable (X) using a polynomial function of X, assuming a non-linear relationship between them. \n",
    "The equation for polynomial regression can have higher order terms of X, such as X^2, X^3, ..., X^n, where n is the degree of the polynomial. \n",
    "The general form of a polynomial regression equation is: Y = β0 + β1X + β2X^2 + ... + βnX^n + ε, where β0, β1, β2, ..., βn are the regression coefficients for each term, X is the independent variable, ε is the error term, and n is the degree of the polynomial.\n",
    "\n",
    "The main difference between polynomial regression and linear regression is that polynomial regression allows for modeling non-linear relationships between the dependent and independent variables, whereas linear regression assumes a linear relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ec620-8799-4dca-9254-4fdc3c8d61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8\n",
    "Advantages of Polynomial Regression:\n",
    "  Flexibility in modeling non-linear relationships: Polynomial regression allows for modeling complex, non-linear relationships between variables, which may not be captured by linear regression. It can capture curved or U-shaped relationships between variables, which can result in more accurate predictions.\n",
    "  Higher order relationships: Polynomial regression can capture higher order relationships, such as quadratic or cubic relationships, which can be important in certain scenarios where the relationship between variables is not linear.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "  Overfitting: Polynomial regression models with high degrees of polynomials are prone to overfitting, especially when the sample size is small. Overfitting occurs when the model fits the noise or random fluctuations in the data, resulting in poor generalization performance on new data.\n",
    "  Complexity: Polynomial regression models with higher degrees of polynomials can become complex and difficult to interpret. They may result in over-parameterization and make it harder to identify the most important variables driving the relationship.\n",
    "\n",
    "In general, polynomial regression may be preferred over linear regression when there is evidence of a non-linear relationship between variables and when higher order relationships need to be captured.\n",
    "However, careful consideration should be given to the potential for overfitting and model complexity, and the appropriateness of the degree of polynomial used should be evaluated based on the specific dataset and problem at hand.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
