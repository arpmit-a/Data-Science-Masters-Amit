{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e64511b-cf3e-4c39-8dde-082386e9fb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bayes' theorem is a fundamental concept in probability theory and statistics. It provides a way to update the probability for a hypothesis (or event) based on new evidence or data. \\nIn essence, it allows us to calculate the probability of a hypothesis being true given what we know about related events or conditions.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "'''Bayes' theorem is a fundamental concept in probability theory and statistics. It provides a way to update the probability for a hypothesis (or event) based on new evidence or data. \n",
    "In essence, it allows us to calculate the probability of a hypothesis being true given what we know about related events or conditions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf70f89-316b-4f5f-acb4-13dab9b4b9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe formula for Bayes' theorem is:\\n\\nP(A∣B)= P(B∣A)⋅P(A)/P(B)\\n\\nWhere:\\n\\nP(A∣B) is the posterior probability of event A given event B.\\n\\nP(B∣A) is the likelihood of event B occurring given event A.\\n\\nP(A) is the prior probability of event A.\\n\\nP(B) is the probability of event B.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "'''\n",
    "The formula for Bayes' theorem is:\n",
    "\n",
    "P(A∣B)= P(B∣A)⋅P(A)/P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A∣B) is the posterior probability of event A given event B.\n",
    "\n",
    "P(B∣A) is the likelihood of event B occurring given event A.\n",
    "\n",
    "P(A) is the prior probability of event A.\n",
    "\n",
    "P(B) is the probability of event B.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1350b74-6d60-4059-bc43-3d80d9b45ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBayes' theorem is widely used in various fields, including statistics, machine learning, and artificial intelligence. It's used for tasks such as:\\n\\nBayesian inference: Updating beliefs or probabilities based on new evidence.\\nClassification: In machine learning, it's used in algorithms like Naive Bayes for text classification, spam detection, and more.\\nMedical diagnosis: Assessing the probability of a disease given certain symptoms and test results.\\nRisk assessment: Estimating the likelihood of events like accidents or financial losses.\\nNatural language processing: Analyzing and predicting language patterns and text sentiment.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "'''\n",
    "Bayes' theorem is widely used in various fields, including statistics, machine learning, and artificial intelligence. It's used for tasks such as:\n",
    "\n",
    "Bayesian inference: Updating beliefs or probabilities based on new evidence.\n",
    "Classification: In machine learning, it's used in algorithms like Naive Bayes for text classification, spam detection, and more.\n",
    "Medical diagnosis: Assessing the probability of a disease given certain symptoms and test results.\n",
    "Risk assessment: Estimating the likelihood of events like accidents or financial losses.\n",
    "Natural language processing: Analyzing and predicting language patterns and text sentiment.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f44af9-1dbd-4180-b084-40bea5c95e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBayes' theorem is a mathematical expression of conditional probability.\\nIt describes how to calculate the conditional probability of one event (A) given another event (B) when we know the conditional probability of B given A and the individual probabilities of A and B. \\nIn other words, it provides a formal way to update our beliefs about the probability of an event based on new information.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "'''\n",
    "Bayes' theorem is a mathematical expression of conditional probability.\n",
    "It describes how to calculate the conditional probability of one event (A) given another event (B) when we know the conditional probability of B given A and the individual probabilities of A and B. \n",
    "In other words, it provides a formal way to update our beliefs about the probability of an event based on new information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8494ac48-5bb5-48ab-b535-d005d5ee1550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe choice of which type of Naive Bayes classifier to use in a given problem depends on the nature of the data and the assumptions you're willing to make about the independence of features. There are three common types of Naive Bayes classifiers:\\n\\nGaussian Naive Bayes: This variant assumes that the continuous features in your data follow a Gaussian (normal) distribution. It's suitable for data with continuous features that are approximately normally distributed.\\n\\nMultinomial Naive Bayes: This variant is commonly used for text classification tasks, where the features represent counts or frequencies of words. It assumes that features are multinomially distributed and often works well for discrete data.\\n\\nBernoulli Naive Bayes: This variant is also used for text classification but is suitable when features are binary (0/1) indicators of the presence or absence of specific words or features.\\n\\nThe choice between these variants depends on the data's characteristics. If you're working with numerical data that doesn't fit a Gaussian distribution, Gaussian Naive Bayes might not be appropriate. \\nSimilarly, if your data is binary or represents counts (as in text data), Bernoulli or Multinomial Naive Bayes may be more suitable.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "'''\n",
    "The choice of which type of Naive Bayes classifier to use in a given problem depends on the nature of the data and the assumptions you're willing to make about the independence of features. There are three common types of Naive Bayes classifiers:\n",
    "\n",
    "Gaussian Naive Bayes: This variant assumes that the continuous features in your data follow a Gaussian (normal) distribution. It's suitable for data with continuous features that are approximately normally distributed.\n",
    "\n",
    "Multinomial Naive Bayes: This variant is commonly used for text classification tasks, where the features represent counts or frequencies of words. It assumes that features are multinomially distributed and often works well for discrete data.\n",
    "\n",
    "Bernoulli Naive Bayes: This variant is also used for text classification but is suitable when features are binary (0/1) indicators of the presence or absence of specific words or features.\n",
    "\n",
    "The choice between these variants depends on the data's characteristics. If you're working with numerical data that doesn't fit a Gaussian distribution, Gaussian Naive Bayes might not be appropriate. \n",
    "Similarly, if your data is binary or represents counts (as in text data), Bernoulli or Multinomial Naive Bayes may be more suitable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9df4e0-1dc2-4ee5-bb78-49dac3dcf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6\n",
    "To predict the class for the new instance with features X1=3 and X2=4 using Naive Bayes, we'll calculate the posterior probabilities for each class (A and B) and choose the class with the highest probability. \n",
    "\n",
    "The Naive Bayes formula for binary classification is:\n",
    "\n",
    "\\[P(Class|X1, X2) \\propto P(Class) \\cdot P(X1|Class) \\cdot P(X2|Class)\\]\n",
    "\n",
    "We are given the following information:\n",
    "\n",
    "- Prior probabilities: P(Class = A) = P(Class = B) = 0.5 (equal prior probabilities).\n",
    "- The frequency of each feature value for each class.\n",
    "\n",
    "Let's calculate the likelihood probabilities P(X1|Class) and P(X2|Class) for each class:\n",
    "\n",
    "For Class A:\n",
    "- P(X1=3|A) = 4/10\n",
    "- P(X2=4|A) = 3/10\n",
    "\n",
    "For Class B:\n",
    "- P(X1=3|B) = 1/7\n",
    "- P(X2=4|B) = 3/7\n",
    "\n",
    "Now, let's calculate the posterior probabilities for each class:\n",
    "\n",
    "For Class A:\n",
    "\\[P(A|X1=3, X2=4) \\propto 0.5 \\cdot (4/10) \\cdot (3/10) = 0.015\\]\n",
    "\n",
    "For Class B:\n",
    "\\[P(B|X1=3, X2=4) \\propto 0.5 \\cdot (1/7) \\cdot (3/7) = 0.010714\\]\n",
    "\n",
    "Since \\(0.015 > 0.010714\\), Naive Bayes would predict that the new instance with features X1=3 and X2=4 belongs to Class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
