{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faa3e1-e52a-496a-80eb-6015350441c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Q1: What is the Filter Method in Feature Selection, and How Does it Work?\n",
    "\n",
    "**Filter Method:**\n",
    "- **Definition:** The filter method evaluates the relevance of features based on statistical measures without involving any machine learning model. It ranks features according to some statistical score and selects the top-ranked features.\n",
    "- **How it Works:**\n",
    "  1. **Calculate Scores:** Compute a relevance score for each feature using statistical techniques such as correlation, mutual information, chi-square test, ANOVA, etc.\n",
    "  2. **Rank Features:** Rank the features based on their scores.\n",
    "  3. **Select Features:** Choose the top-ranked features as per the desired number of features or a threshold score.\n",
    "\n",
    "**Example in Python:**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Apply SelectKBest with chi2 score function\n",
    "selector = SelectKBest(score_func=chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected features:\\n\", X_new)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818c4d0-68eb-4350-9af9-5ea2be12982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q2: How Does the Wrapper Method Differ from the Filter Method in Feature Selection?\n",
    "\n",
    "**Wrapper Method:**\n",
    "- **Definition:** The wrapper method evaluates subsets of features by training a model on them and selecting the subset that gives the best performance.\n",
    "- **Differences from Filter Method:**\n",
    "  - **Model Involvement:** Wrapper methods use a predictive model to evaluate feature subsets, while filter methods rely solely on statistical measures.\n",
    "  - **Performance:** Wrapper methods typically yield better performance as they consider feature interactions, but they are computationally more expensive.\n",
    "  - **Subset Evaluation:** Wrapper methods search through various combinations of features to find the optimal subset, whereas filter methods rank features individually.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9a7b1-6237-4f35-b848-978cf9a3c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q3: What are Some Common Techniques Used in Embedded Feature Selection Methods?\n",
    "\n",
    "**Embedded Feature Selection:**\n",
    "- Embedded methods perform feature selection during the model training process. They are part of the model-building process and often more efficient than filter and wrapper methods.\n",
    "- **Common Techniques:**\n",
    "  - **Lasso (L1 Regularization):** Penalizes the absolute sum of coefficients, driving some coefficients to zero.\n",
    "  - **Ridge (L2 Regularization):** Penalizes the squared sum of coefficients, though it doesnâ€™t perform feature selection but regularizes the coefficients.\n",
    "  - **Elastic Net:** Combines L1 and L2 regularization.\n",
    "  - **Tree-Based Methods:** Feature importance scores in decision trees and ensemble methods (e.g., Random Forest, Gradient Boosting).\n",
    "\n",
    "**Example in Python:**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load dataset\n",
    "data = load_boston()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Apply Lasso for feature selection\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get coefficients\n",
    "print(\"Feature coefficients:\\n\", lasso.coef_)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052abc9-b0ea-4d59-8e0f-bc644066f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q4: What are Some Drawbacks of Using the Filter Method for Feature Selection?\n",
    "\n",
    "- **Ignores Feature Interactions:** The filter method evaluates features individually, which might miss important interactions between features.\n",
    "- **Less Accurate:** As it does not involve a predictive model, it might not select the most relevant features for improving model performance.\n",
    "- **Fixed Thresholds:** The relevance scores are based on fixed statistical measures, which might not be optimal for all datasets or tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee6ecd-9db9-4b37-b5dd-d615d0e2b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q5: In Which Situations Would You Prefer Using the Filter Method Over the Wrapper Method for Feature Selection?\n",
    "\n",
    "- **Large Datasets:** When dealing with large datasets where computational efficiency is crucial.\n",
    "- **Preprocessing Step:** As an initial step to quickly remove irrelevant or redundant features before applying more computationally intensive methods.\n",
    "- **Exploratory Analysis:** When conducting an exploratory analysis to get a quick understanding of the feature relevance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7036fbc-71d6-48d1-9ca9-b7e4d741c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q6: Using the Filter Method to Choose Pertinent Attributes for Customer Churn Prediction in a Telecom Company\n",
    "\n",
    "**Steps:**\n",
    "1. **Load Dataset:** Load the telecom dataset.\n",
    "2. **Calculate Relevance Scores:** Use statistical methods (e.g., chi-square test, mutual information) to calculate relevance scores for each feature with respect to the target (customer churn).\n",
    "3. **Rank Features:** Rank the features based on the calculated scores.\n",
    "4. **Select Top Features:** Select the top-ranked features for model training.\n",
    "\n",
    "**Example in Python:**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset (assuming df is your DataFrame with 'churn' as the target variable)\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "# Apply SelectKBest with chi2 score function\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected features:\\n\", X_new)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef592a-f773-4f5a-8e09-7c73bff73992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d15ef-e76a-430c-81d8-fa8a6dde24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q7: Using the Embedded Method to Select Relevant Features for Predicting Soccer Match Outcomes\n",
    "\n",
    "**Steps:**\n",
    "1. **Load Dataset:** Load the soccer dataset containing player statistics and team rankings.\n",
    "2. **Choose Embedded Method:** Use a method like Lasso regression or a tree-based model.\n",
    "3. **Train Model:** Train the model and obtain feature importance or coefficients.\n",
    "4. **Select Features:** Select the features based on their importance scores or non-zero coefficients.\n",
    "\n",
    "**Example in Python:**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset (assuming df is your DataFrame with 'outcome' as the target variable)\n",
    "X = df.drop('outcome', axis=1)\n",
    "y = df['outcome']\n",
    "\n",
    "# Apply Lasso for feature selection\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get coefficients\n",
    "print(\"Feature coefficients:\\n\", lasso.coef_)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa31c1-3d82-44f9-8712-67acac3e0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q8: Using the Wrapper Method to Select the Best Set of Features for Predicting House Prices\n",
    "\n",
    "**Steps:**\n",
    "1. **Load Dataset:** Load the house pricing dataset.\n",
    "2. **Define Model:** Choose a predictive model (e.g., linear regression).\n",
    "3. **Feature Subset Evaluation:** Use a method like recursive feature elimination (RFE) to evaluate different subsets of features.\n",
    "4. **Select Best Subset:** Select the subset that yields the best performance based on model evaluation metrics.\n",
    "\n",
    "**Example in Python:**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset (assuming df is your DataFrame with 'price' as the target variable)\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Define model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Apply RFE for feature selection\n",
    "selector = RFE(model, n_features_to_select=5)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# Get selected features\n",
    "print(\"Selected features:\\n\", selector.support_)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
