{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959df1e-322f-41c4-8adf-3ad5bc37a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "- Overfitting:\n",
    "  Overfitting occurs when a machine learning model learns the training data too well, including the noise and outliers. This leads to a model that performs exceptionally well on the training data but poorly on new, unseen data.\n",
    "  - Consequences: Poor generalization to new data, high variance.\n",
    "  - Mitigation: Cross-validation, regularization (L1, L2), pruning (for decision trees), simplifying the model, reducing the number of features, and gathering more training data.\n",
    "\n",
    "- Underfitting:\n",
    "  Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. This leads to poor performance on both training and new data.\n",
    "  - Consequences: High bias, poor predictive performance.\n",
    "  - Mitigation: Increasing the complexity of the model, using more features, reducing regularization, and increasing the training time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd6338-1e87-4516-a99e-2b8728b8cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q2: How can we reduce overfitting? Explain in brief.\n",
    "- Cross-Validation: Splitting the data into multiple folds and training the model on different folds to ensure it generalizes well.\n",
    "- Regularization: Adding a penalty to the model's complexity (L1, L2 regularization).\n",
    "- Pruning: Removing parts of a model that may cause overfitting, such as trimming branches of a decision tree.\n",
    "- Simplifying the Model: Using a simpler model with fewer parameters.\n",
    "- Early Stopping: Halting the training process before the model becomes too complex.\n",
    "- Dropout (for Neural Networks): Randomly dropping neurons during training to prevent co-adaptation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159645ca-6381-4e95-85cc-8ad5149d88d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "- Underfitting: When a model is too simple to capture the underlying structure of the data.\n",
    "- Scenarios:\n",
    "  - Using a linear model for a non-linear problem.\n",
    "  - Having insufficient training data.\n",
    "  - Using overly aggressive regularization.\n",
    "  - Insufficient training time for complex models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d0424-6aad-42a7-a1dc-67afadb7d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "- Bias: The error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause underfitting.\n",
    "- Variance: The error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause overfitting.\n",
    "- Tradeoff: Increasing model complexity reduces bias but increases variance, and vice versa. The goal is to find a balance where both bias and variance are minimized, leading to good generalization performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bcdecf-e256-42c5-bfe0-abd462bfe51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "- Detecting Overfitting:\n",
    "  - High accuracy on training data but low accuracy on validation/test data.\n",
    "  - Large difference between training and validation errors.\n",
    "  - Use learning curves to visualize training and validation performance.\n",
    "- Detecting Underfitting:\n",
    "  - Low accuracy on both training and validation/test data.\n",
    "  - Small difference between training and validation errors.\n",
    "  - Use learning curves to visualize training and validation performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6413d7-29d4-4628-8160-cefc17ac5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "- High Bias Models:\n",
    "  - Linear regression, Logistic regression with insufficient features.\n",
    "  - Simple models that are likely to underfit.\n",
    "  - Tend to have high training and test errors.\n",
    "- High Variance Models:\n",
    "  - Decision trees without pruning, highly complex neural networks.\n",
    "  - Complex models that are likely to overfit.\n",
    "  - Tend to have low training error but high test error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73cac2-9cdf-4faf-a8b6-8796658c22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "- Regularization: A technique used to reduce the complexity of the model by adding a penalty to the loss function.\n",
    "- Common Techniques:\n",
    "  - L1 Regularization (Lasso): Adds the absolute value of the coefficients as a penalty term to the loss function, leading to sparse models where some coefficients are zero.\n",
    "  - L2 Regularization (Ridge): Adds the squared value of the coefficients as a penalty term to the loss function, leading to smaller but non-zero coefficients.\n",
    "  - Elastic Net: Combines L1 and L2 regularization.\n",
    "  - Dropout (for Neural Networks): Randomly drops a fraction of neurons during training to prevent overfitting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
