{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f81e0-9ebb-4bab-be2b-3b555500ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. R-squared in linear regression models:\n",
    "\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of variance in the dependent variable that is predictable from the independent variable(s) in a regression model.\n",
    "\n",
    "Calculation:\n",
    "R² = 1 - (SSR / SST)\n",
    "\n",
    "Where:\n",
    "SSR = Sum of Squared Residuals (unexplained variance)\n",
    "SST = Total Sum of Squares (total variance)\n",
    "\n",
    "R-squared ranges from 0 to 1, where:\n",
    "- 0 indicates that the model explains none of the variability in the data\n",
    "- 1 indicates that the model explains all the variability\n",
    "\n",
    "It represents how well the regression model fits the observed data. A higher R-squared suggests a better fit of the model to the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82f6cd-97e6-47f9-976a-76efe674bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of predictors in the model.\n",
    "\n",
    "Calculation:\n",
    "Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Where:\n",
    "n = number of observations\n",
    "k = number of predictors\n",
    "\n",
    "The main difference is that adjusted R-squared penalizes the addition of unnecessary predictors to the model, while regular R-squared always increases (or stays the same) when more predictors are added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075bfd1-7dc7-4b58-b0fd-021370ea3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When to use adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is more appropriate in the following situations:\n",
    "\n",
    "1. When comparing models with different numbers of predictors\n",
    "2. In multiple regression analysis with many predictors\n",
    "3. When you want to avoid overfitting by penalizing excessive complexity\n",
    "4. When you need to account for the trade-off between model complexity and goodness of fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4cc6e-7388-4fa5-88cc-4075c627ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. RMSE, MSE, and MAE in regression analysis:\n",
    "\n",
    "RMSE (Root Mean Square Error):\n",
    "- Calculation: √(Σ(actual - predicted)² / n)\n",
    "- Represents the standard deviation of the residuals (prediction errors)\n",
    "\n",
    "MSE (Mean Square Error):\n",
    "- Calculation: Σ(actual - predicted)² / n\n",
    "- Represents the average squared difference between predicted and actual values\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "- Calculation: Σ|actual - predicted| / n\n",
    "- Represents the average absolute difference between predicted and actual values\n",
    "\n",
    "In all cases, n is the number of observations. These metrics measure the average magnitude of prediction errors in a regression model, with lower values indicating better model performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eba0a3-0055-4df6-876b-2a6f5652b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Advantages and disadvantages of RMSE, MSE, and MAE:\n",
    "\n",
    "RMSE:\n",
    "Advantages:\n",
    "- Same unit as the dependent variable\n",
    "- Penalizes large errors more heavily\n",
    "- Useful when large errors are particularly undesirable\n",
    "\n",
    "Disadvantages:\n",
    "- More sensitive to outliers\n",
    "- Can be harder to interpret than MAE\n",
    "\n",
    "MSE:\n",
    "Advantages:\n",
    "- Penalizes large errors more heavily\n",
    "- Mathematically convenient for optimization\n",
    "\n",
    "Disadvantages:\n",
    "- Not in the same unit as the dependent variable\n",
    "- Can be less interpretable than RMSE or MAE\n",
    "\n",
    "MAE:\n",
    "Advantages:\n",
    "- Same unit as the dependent variable\n",
    "- Less sensitive to outliers\n",
    "- Easier to interpret\n",
    "\n",
    "Disadvantages:\n",
    "- Doesn't penalize large errors as heavily as RMSE or MSE\n",
    "- May not be suitable when large errors are particularly problematic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ddf9c-1990-490a-a80d-eb30913eee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Lasso regularization:\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used to prevent overfitting in regression models by adding a penalty term to the loss function.\n",
    "\n",
    "Lasso regularization adds the sum of the absolute values of the coefficients multiplied by a regularization parameter (λ) to the loss function:\n",
    "\n",
    "Loss = MSE + λ * Σ|βi|\n",
    "\n",
    "Where βi are the model coefficients.\n",
    "\n",
    "Differences from Ridge regularization:\n",
    "1. Lasso uses L1 regularization (absolute value), while Ridge uses L2 regularization (squared values)\n",
    "2. Lasso can lead to sparse models by forcing some coefficients to zero, effectively performing feature selection\n",
    "3. Ridge tends to shrink all coefficients toward zero but rarely makes them exactly zero\n",
    "\n",
    "Lasso is more appropriate when:\n",
    "1. You want to perform feature selection\n",
    "2. You suspect only a subset of features are relevant\n",
    "3. You prefer a simpler, more interpretable model\n",
    "4. You're dealing with high-dimensional data with many irrelevant features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69a1e9-a380-490c-b1ca-a12290feebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How regularized linear models prevent overfitting:\n",
    "\n",
    "Regularized linear models prevent overfitting by adding a penalty term to the loss function, which discourages the model from relying too heavily on any single feature or learning noise in the training data.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with 100 features, but only 10 are actually relevant to predicting the target variable. Without regularization, a linear model might assign non-zero coefficients to all 100 features, potentially fitting noise in the training data.\n",
    "\n",
    "With Lasso regularization:\n",
    "1. The model is penalized for using large coefficient values\n",
    "2. Many of the coefficients for irrelevant features are pushed to zero\n",
    "3. The resulting model is simpler and generalizes better to new data\n",
    "4. Only the most important features retain non-zero coefficients\n",
    "\n",
    "This process helps prevent overfitting by reducing model complexity and focusing on the most relevant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b9601-52c8-494d-b934-d0aba1eaf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Limitations of regularized linear models:\n",
    "\n",
    "1. Assumption of linearity: They may not capture complex, non-linear relationships in the data\n",
    "2. Feature scaling sensitivity: Regularization is sensitive to the scale of features, requiring careful preprocessing\n",
    "3. Hyperparameter tuning: Choosing the optimal regularization strength can be challenging and time-consuming\n",
    "4. Interpretability trade-off: While regularization can improve generalization, it may make coefficient interpretation more difficult\n",
    "5. Multicollinearity handling: Ridge regression handles multicollinearity better than Lasso in some cases\n",
    "6. Limited to additive effects: They don't naturally capture interaction effects between features\n",
    "7. Outlier sensitivity: They can still be influenced by outliers, especially in the case of Ridge regression\n",
    "8. Assumption of independent errors: They assume errors are independent, which may not always hold true\n",
    "\n",
    "Due to these limitations, regularized linear models may not be the best choice when:\n",
    "- The relationship between features and target is highly non-linear\n",
    "- There are strong interaction effects between features\n",
    "- The data violates assumptions of linearity and independence\n",
    "- Extremely high accuracy is required for complex problems\n",
    "\n",
    "In such cases, more advanced techniques like decision trees, random forests, or neural networks might be more appropriate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a67a3-af54-4b19-9dbe-842e287cc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Comparing Model A (RMSE = 10) and Model B (MAE = 8):\n",
    "\n",
    "This comparison is challenging because RMSE and MAE are different metrics and are not directly comparable. However, we can make some observations:\n",
    "\n",
    "1. RMSE is always greater than or equal to MAE for the same set of predictions.\n",
    "2. The difference between RMSE and MAE grows as the variance of the errors increases.\n",
    "\n",
    "Given this information, we cannot definitively say which model is better without more context. However, we can discuss the implications:\n",
    "\n",
    "1. If the error distributions are similar for both models, Model B might be preferable because MAE of 8 suggests that, on average, predictions are off by 8 units.\n",
    "2. Model A's RMSE of 10 suggests that there might be some larger errors, as RMSE penalizes large errors more heavily.\n",
    "\n",
    "Limitations of this comparison:\n",
    "1. Different metrics: We're comparing apples to oranges, which is not ideal.\n",
    "2. Lack of context: We don't know the scale of the target variable or the nature of the problem.\n",
    "3. Missing information: We don't have both metrics for each model, which would allow for a more comprehensive comparison.\n",
    "\n",
    "To make a more informed decision, we should:\n",
    "1. Calculate both RMSE and MAE for both models\n",
    "2. Consider the specific requirements of the problem (e.g., are large errors particularly problematic?)\n",
    "3. Examine other metrics like R-squared or adjusted R-squared\n",
    "4. Look at the residual plots to understand the error distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e40af9-6168-4799-8977-e6a6a95c4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. Comparing Model A (Ridge, λ=0.1) and Model B (Lasso, λ=0.5):\n",
    "\n",
    "Choosing between these models depends on various factors:\n",
    "\n",
    "1. Regularization strength: The λ values are different, making direct comparison difficult. A higher λ generally means stronger regularization.\n",
    "\n",
    "2. Feature selection:\n",
    "   - Lasso (Model B) tends to produce sparse models by setting some coefficients to exactly zero.\n",
    "   - Ridge (Model A) shrinks coefficients but rarely sets them to exactly zero.\n",
    "\n",
    "3. Multicollinearity:\n",
    "   - Ridge performs better when features are highly correlated.\n",
    "   - Lasso might arbitrarily choose one of several correlated features.\n",
    "\n",
    "4. Model interpretability:\n",
    "   - Lasso can lead to more interpretable models due to feature selection.\n",
    "   - Ridge keeps all features but with smaller coefficients.\n",
    "\n",
    "5. Prediction accuracy:\n",
    "   - We don't have information about the models' performance on validation data.\n",
    "\n",
    "Trade-offs and limitations:\n",
    "\n",
    "1. Without performance metrics, we can't determine which model generalizes better.\n",
    "2. The choice depends on the specific problem and dataset characteristics.\n",
    "3. Different λ values make it hard to compare the regularization effects directly.\n",
    "4. We don't know if the λ values were optimized for each method.\n",
    "\n",
    "To make a better decision:\n",
    "\n",
    "1. Compare performance metrics (e.g., RMSE, MAE, R-squared) on validation data.\n",
    "2. Use cross-validation to optimize λ for each method.\n",
    "3. Consider the problem requirements (e.g., need for feature selection, interpretability).\n",
    "4. Examine the coefficient values and their stability across different samples.\n",
    "5. Try Elastic Net, which combines both Ridge and Lasso, to leverage benefits of both.\n",
    "\n",
    "In conclusion, the choice between these models depends on the specific problem context, dataset characteristics, and performance on validation data. Without more information, we cannot definitively choose one over the other.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
