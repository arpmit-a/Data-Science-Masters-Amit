{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59873229-17e0-4cdb-b04e-df0e20761aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "KNN (K-Nearest Neighbors) is a simple, non-parametric machine learning algorithm used for both classification and regression tasks. The main idea behind KNN is:\n",
    "\n",
    "1. For a given data point, find the K nearest neighbors in the training dataset.\n",
    "2. For classification: Assign the majority class among these K neighbors.\n",
    "3. For regression: Take the average (or weighted average) of the target values of these K neighbors.\n",
    "\n",
    "Key characteristics:\n",
    "- Instance-based learning: It doesn't build an explicit model, but uses the entire training set for predictions.\n",
    "- Lazy learning: No training phase; all computation is done at prediction time.\n",
    "- Non-parametric: It doesn't make assumptions about the underlying data distribution.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing the optimal K value is crucial for KNN performance. Methods include:\n",
    "\n",
    "1. Cross-validation: Use k-fold cross-validation to test different K values and choose the one with the best performance.\n",
    "\n",
    "2. Elbow method: Plot the error rate against different K values and look for the \"elbow\" point where the error rate starts to level off.\n",
    "\n",
    "3. Square root method: Use the square root of the number of samples in the training dataset as K.\n",
    "\n",
    "4. Domain knowledge: Consider the noise level and complexity of the problem domain.\n",
    "\n",
    "5. Odd vs. Even: For binary classification, use odd numbers to avoid ties.\n",
    "\n",
    "6. Hyperparameter tuning: Use techniques like grid search or random search to find the optimal K.\n",
    "\n",
    "Considerations:\n",
    "- Smaller K: More sensitive to noise, but can capture fine-grained patterns.\n",
    "- Larger K: More robust to noise, but might miss local patterns.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940082f0-f20e-4815-bd2a-6425398ebe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Difference between KNN classifier and KNN regressor:\n",
    "\n",
    "KNN Classifier:\n",
    "- Used for categorical target variables\n",
    "- Predicts the class label based on majority voting among K neighbors\n",
    "- Output is a discrete class label\n",
    "\n",
    "KNN Regressor:\n",
    "- Used for continuous target variables\n",
    "- Predicts the target value by averaging (or weighted averaging) the values of K neighbors\n",
    "- Output is a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86c606-d8d6-4c14-920a-1f982c024463",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Measuring the performance of KNN:\n",
    "\n",
    "For KNN Classifier:\n",
    "1. Accuracy: Proportion of correct predictions\n",
    "2. Precision, Recall, F1-score: Especially useful for imbalanced datasets\n",
    "3. Confusion Matrix: Visualizes prediction errors\n",
    "4. ROC-AUC: Area under the Receiver Operating Characteristic curve\n",
    "\n",
    "For KNN Regressor:\n",
    "1. Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)\n",
    "2. Mean Absolute Error (MAE)\n",
    "3. R-squared (coefficient of determination)\n",
    "\n",
    "For both:\n",
    "- Cross-validation scores: To assess model stability and generalization\n",
    "- Learning curves: To diagnose bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc78c3-71a8-4454-96a7-68a8e734abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q5. The curse of dimensionality in KNN:\n",
    "\n",
    "The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces. For KNN, it manifests as:\n",
    "\n",
    "1. Increased sparsity: As dimensions increase, the available data becomes sparse.\n",
    "2. Distance concentration: Distances between points become less meaningful in high dimensions.\n",
    "3. Computational complexity: Finding nearest neighbors becomes more time-consuming.\n",
    "4. Overfitting: With high dimensions, points can become equidistant, leading to poor generalization.\n",
    "\n",
    "Addressing the curse:\n",
    "- Feature selection: Reduce the number of features\n",
    "- Dimensionality reduction techniques: PCA, t-SNE, etc.\n",
    "- Using distance metrics suited for high dimensions\n",
    "- Increasing the size of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42944a-712d-48ea-b6a9-057f6b714d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Handling missing values in KNN:\n",
    "\n",
    "Strategies for handling missing values in KNN include:\n",
    "\n",
    "1. Complete case analysis: Remove instances with missing values (not recommended for large amounts of missing data).\n",
    "\n",
    "2. Mean/median imputation: Replace missing values with the mean or median of the feature.\n",
    "\n",
    "3. KNN imputation: Use KNN algorithm itself to impute missing values based on similar instances.\n",
    "\n",
    "4. Multiple imputation: Create multiple plausible imputed datasets and combine results.\n",
    "\n",
    "5. Indicator variables: Create binary indicators for missingness alongside imputed values.\n",
    "\n",
    "6. Advanced imputation methods: MICE (Multivariate Imputation by Chained Equations) or other sophisticated techniques.\n",
    "\n",
    "7. Domain-specific imputation: Use domain knowledge to inform imputation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4adcd4f-b85e-4f2c-854b-4e3290c864f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Comparing KNN classifier and regressor performance:\n",
    "\n",
    "KNN Classifier:\n",
    "- Better for: Categorical outcomes, decision boundaries, multi-class problems\n",
    "- Strengths: Handles non-linear decision boundaries well, easy to interpret\n",
    "- Weaknesses: Sensitive to imbalanced datasets, may struggle with high-dimensional data\n",
    "\n",
    "KNN Regressor:\n",
    "- Better for: Continuous outcomes, smooth functions, interpolation\n",
    "- Strengths: Can capture local patterns in the data, works well for non-linear relationships\n",
    "- Weaknesses: Sensitive to outliers, may struggle with extrapolation\n",
    "\n",
    "Choice depends on:\n",
    "- Nature of the target variable (categorical vs. continuous)\n",
    "- Data distribution and underlying relationships\n",
    "- Problem requirements (e.g., interpretability, handling of outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ad4a7-3f09-499b-8463-d33b2857132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Strengths and weaknesses of KNN for classification and regression:\n",
    "\n",
    "Strengths:\n",
    "1. Simple and intuitive\n",
    "2. No assumptions about data distribution\n",
    "3. Can model complex, non-linear decision boundaries\n",
    "4. Naturally handles multi-class problems\n",
    "5. Easy to implement and interpret\n",
    "\n",
    "Weaknesses:\n",
    "1. Computationally expensive for large datasets\n",
    "2. Sensitive to irrelevant features and the curse of dimensionality\n",
    "3. Requires feature scaling\n",
    "4. Memory-intensive (stores all training data)\n",
    "5. Sensitive to imbalanced datasets\n",
    "\n",
    "Addressing weaknesses:\n",
    "- Use approximate nearest neighbor algorithms for large datasets\n",
    "- Apply feature selection or dimensionality reduction\n",
    "- Implement proper feature scaling\n",
    "- Use weighted KNN to handle imbalanced data\n",
    "- Consider using tree-based KNN implementations for better efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015fc08-04aa-4359-ae19-89656432080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Difference between Euclidean distance and Manhattan distance in KNN:\n",
    "\n",
    "Euclidean distance:\n",
    "- Formula: sqrt(Σ(x_i - y_i)^2)\n",
    "- Represents the straight-line distance between two points\n",
    "- Better for continuous, smooth spaces\n",
    "- More sensitive to outliers\n",
    "\n",
    "Manhattan distance:\n",
    "- Formula: Σ|x_i - y_i|\n",
    "- Represents the sum of absolute differences between coordinates\n",
    "- Better for discrete or grid-like spaces\n",
    "- Less sensitive to outliers\n",
    "\n",
    "Choice depends on:\n",
    "- Nature of the feature space\n",
    "- Presence of outliers\n",
    "- Computational efficiency requirements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13100c-b97b-47a2-8d70-541163dbd52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. Role of feature scaling in KNN:\n",
    "\n",
    "Feature scaling is crucial in KNN because:\n",
    "\n",
    "1. Distance-based algorithm: KNN relies on distance calculations, which are affected by the scale of features.\n",
    "\n",
    "2. Equal importance: Scaling ensures all features contribute equally to distance calculations.\n",
    "\n",
    "3. Prevent domination: Without scaling, features with larger magnitudes would dominate the distance calculation.\n",
    "\n",
    "4. Improved performance: Proper scaling often leads to better model performance and faster convergence.\n",
    "\n",
    "5. Consistency: Scaling provides a consistent basis for comparing and interpreting distances.\n",
    "\n",
    "Common scaling methods:\n",
    "- Min-Max scaling: Scales features to a fixed range, usually [0, 1]\n",
    "- Standard scaling: Transforms features to have zero mean and unit variance\n",
    "- Robust scaling: Uses median and interquartile range, less sensitive to outliers\n",
    "\n",
    "Best practices:\n",
    "- Apply scaling to both training and test data\n",
    "- Use the same scaling parameters for both sets\n",
    "- Consider the nature of the data when choosing a scaling method\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
