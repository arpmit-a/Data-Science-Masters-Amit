{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446bb8b-7bfe-4f32-9290-b6bc7d1a08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "I'd be happy to help you with your assignment questions about Support Vector Machines (SVMs). Let's go through them one by one:\n",
    "\n",
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The linear SVM formula for classification can be expressed as:\n",
    "\n",
    "f(x) = w^T x + b\n",
    "\n",
    "Where:\n",
    "- w is the weight vector\n",
    "- x is the input vector\n",
    "- b is the bias term\n",
    "- f(x) is the decision function\n",
    "\n",
    "The classification rule is:\n",
    "- If f(x) ≥ 0, classify as positive class\n",
    "- If f(x) < 0, classify as negative class\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee5bdb-819c-45e8-8d16-f570d3b49d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function for a linear SVM aims to maximize the margin while minimizing classification errors. It can be written as:\n",
    "\n",
    "minimize: (1/2)||w||^2 + C * Σ ξᵢ\n",
    "\n",
    "subject to: yᵢ(w^T xᵢ + b) ≥ 1 - ξᵢ and ξᵢ ≥ 0 for all i\n",
    "\n",
    "Where:\n",
    "- ||w||^2 is the L2 norm of the weight vector\n",
    "- C is the regularization parameter\n",
    "- ξᵢ are slack variables for handling non-linearly separable data\n",
    "- yᵢ are the class labels (+1 or -1)\n",
    "- xᵢ are the input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd55665-9636-4d60-a51b-82d6a74a70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q3: Kernel Trick in SVM\n",
    "The kernel trick allows SVM to classify data that is not linearly separable by implicitly mapping the input features into a higher-dimensional space. The kernel function computes the dot product in this higher-dimensional space without explicitly transforming the data, which makes the computation efficient.\n",
    "\n",
    "Common kernel functions include:\n",
    "- Linear kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j \\)\n",
    "- Polynomial kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + 1)^d \\)\n",
    "- Radial Basis Function (RBF) kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2) \\)\n",
    "\n",
    "### Q4: Role of Support Vectors in SVM\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane). These points are crucial because they define the position and orientation of the hyperplane. In other words, the support vectors are the critical elements of the dataset that affect the shape of the margin.\n",
    "\n",
    "Example:\n",
    "Consider a simple 2D dataset with two classes. The support vectors are the data points that lie on the margin boundaries. If you move any of these support vectors, the optimal hyperplane will change, but moving any other data point that is not a support vector will not affect the hyperplane.\n",
    "\n",
    "### Q5: Illustrations with Examples and Graphs\n",
    "Let's create examples and graphs to illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# Generate a simple 2D dataset\n",
    "X, y = datasets.make_blobs(n_samples=50, centers=2, random_state=6)\n",
    "\n",
    "# Fit the model\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary, support vectors, and margin\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "            facecolors='none', edgecolors='k', marker='o', label='Support Vectors')\n",
    "\n",
    "# Plot the decision boundary and margins\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "plt.title('SVM with Hard Margin')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Soft margin example with C=0.1 (allowing some misclassifications)\n",
    "clf_soft = svm.SVC(kernel='linear', C=0.1)\n",
    "clf_soft.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "plt.scatter(clf_soft.support_vectors_[:, 0], clf_soft.support_vectors_[:, 1], s=100,\n",
    "            facecolors='none', edgecolors='k', marker='o', label='Support Vectors')\n",
    "\n",
    "# Plot the decision boundary and margins\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf_soft.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "plt.title('SVM with Soft Margin (C=0.1)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Q6: SVM Implementation Through Iris Dataset\n",
    "\n",
    "```python\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Only take the first two features for visualization purposes\n",
    "y = iris.target\n",
    "\n",
    "# Only consider binary classification (class 0 and class 1)\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Train the SVM model\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "            facecolors='none', edgecolors='k', marker='o', label='Support Vectors')\n",
    "\n",
    "# Plot the decision boundary and margins\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "plt.title('SVM on Iris Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Bonus Task: Implementing a Linear SVM Classifier from Scratch\n",
    "\n",
    "```python\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.learning_rate * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Only take the first two features for visualization purposes\n",
    "y = iris.target\n",
    "\n",
    "# Only consider binary classification (class 0 and class 1)\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Train the Linear SVM model from scratch\n",
    "linear_svm = LinearSVM()\n",
    "linear_svm.fit(X, y)\n",
    "y_pred_scratch = linear_svm.predict(X)\n",
    "\n",
    "# Train the scikit-learn SVM model\n",
    "clf = svm.SVC(kernel='linear', C\n",
    "\n",
    "=1)\n",
    "clf.fit(X, y)\n",
    "y_pred_sklearn = clf.predict(X)\n",
    "\n",
    "# Compare performance\n",
    "accuracy_scratch = np.mean(y_pred_scratch == y)\n",
    "accuracy_sklearn = np.mean(y_pred_sklearn == y)\n",
    "print(f'Accuracy (from scratch): {accuracy_scratch}')\n",
    "print(f'Accuracy (scikit-learn): {accuracy_sklearn}')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
